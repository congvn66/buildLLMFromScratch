{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMbhP/h4poiVt8H1+1bsPOi"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#Token Embeddings:"],"metadata":{"id":"L6_qvKBXpKfR"}},{"cell_type":"markdown","source":["To capture semantic meaning between words."],"metadata":{"id":"-rHfguwApPDY"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"Bx5APRdSpFW0","executionInfo":{"status":"ok","timestamp":1754102362983,"user_tz":-420,"elapsed":8720,"user":{"displayName":"Công Nguyễn Đức","userId":"18003168473153234789"}}},"outputs":[],"source":["import torch\n","\n","example_tensor = torch.tensor([2, 4, 1, 5])"]},{"cell_type":"markdown","source":["Actually, the semantic stuffs are captured by the dot product.\n","\n","We turn the token IDs into vectors, that's how.\n","\n","And now, we will look at embedding layers, which we are not understand fully of it."],"metadata":{"id":"r7_JuO2eplyz"}},{"cell_type":"code","source":["num_rows = 6 # number of words in the vocabulary\n","num_cols = 4 # number of dimension (or feature that you can relate words and words) for the vectors\n","\n","# the embedded vectors will be the row vectors of the embedding layer's weight matrix.\n","\n","embeding_layer = torch.nn.Embedding(num_rows, num_cols)\n","embeding_layer.weight"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ekpfo9cGpbfA","executionInfo":{"status":"ok","timestamp":1754102678708,"user_tz":-420,"elapsed":79,"user":{"displayName":"Công Nguyễn Đức","userId":"18003168473153234789"}},"outputId":"a92cf1dc-0240-47e9-dda1-a0150d3216d6"},"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Parameter containing:\n","tensor([[-0.9443,  1.4978,  0.0848, -0.8216],\n","        [ 1.0317, -0.1979,  0.3317,  1.3240],\n","        [ 0.1967,  0.4902,  0.8437, -0.4315],\n","        [ 0.2057,  0.9836,  0.9729, -1.4729],\n","        [ 0.3787, -1.5212, -1.9263, -2.2723],\n","        [-0.2229, -0.7597,  0.4349,  1.3056]], requires_grad=True)"]},"metadata":{},"execution_count":3}]},{"cell_type":"code","source":["embeding_layer(example_tensor)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rMzLMN8uqlFt","executionInfo":{"status":"ok","timestamp":1754102707768,"user_tz":-420,"elapsed":18,"user":{"displayName":"Công Nguyễn Đức","userId":"18003168473153234789"}},"outputId":"6e14da3d-0953-4a35-9263-29df51a723aa"},"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[ 0.1967,  0.4902,  0.8437, -0.4315],\n","        [ 0.3787, -1.5212, -1.9263, -2.2723],\n","        [ 1.0317, -0.1979,  0.3317,  1.3240],\n","        [-0.2229, -0.7597,  0.4349,  1.3056]], grad_fn=<EmbeddingBackward0>)"]},"metadata":{},"execution_count":4}]},{"cell_type":"markdown","source":["Then, finally, we will train the embedding layer and the \"next word guessing\" stuffs in the process of training LLM."],"metadata":{"id":"zgemhnLTq10z"}},{"cell_type":"code","source":[],"metadata":{"id":"P-7WRZaZrImc"},"execution_count":null,"outputs":[]}]}