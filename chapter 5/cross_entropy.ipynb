{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "tLRCFsrk3q1T"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class GPT2Model(nn.Module):\n",
        "  def __init__(self, cfg_map):\n",
        "    super().__init__()\n",
        "\n",
        "    # embedding components\n",
        "    self.emb_layer = nn.Embedding(cfg_map['vocab_size'], cfg_map['emb_dim'])\n",
        "    self.pos_emb_layer = nn.Embedding(cfg_map['context_length'], cfg_map['emb_dim'])\n",
        "\n",
        "    # huh\n",
        "    self.dropout = nn.Dropout(cfg_map['drop_rate'])\n",
        "\n",
        "    # transformer\n",
        "    self.trfm_block = nn.Sequential(*[TransformerBlock(cfg_map) for i in range(cfg_map['n_layers'])])\n",
        "\n",
        "    self.final_norm = LayerNorm(cfg_map['emb_dim'])\n",
        "\n",
        "    # convert to logits\n",
        "    self.out_head = nn.Linear(cfg_map['emb_dim'], cfg_map['vocab_size'], bias = False)\n",
        "\n",
        "  def forward(self, in_idx):\n",
        "    batch_size, seq_len = in_idx.shape\n",
        "    tok_embed = self.emb_layer(in_idx)\n",
        "    pos_embed = self.pos_emb_layer(torch.arange(seq_len, device = in_idx.device))\n",
        "    x = tok_embed + pos_embed\n",
        "    x = self.dropout(x)\n",
        "    x = self.trfm_block(x)\n",
        "    x = self.final_norm(x)\n",
        "\n",
        "    logits = self.out_head(x)\n",
        "\n",
        "    return logits\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "  def __init__(self, cfg_map):\n",
        "    super().__init__()\n",
        "    self.norm_1 = LayerNorm(cfg_map['emb_dim'])\n",
        "    self.multihead_attention = MultiheadAttention(cfg_map['emb_dim'], cfg_map['emb_dim'], cfg_map['drop_rate'], cfg_map['context_length'], cfg_map['n_heads'])\n",
        "    self.dropout = nn.Dropout(cfg_map['drop_rate'])\n",
        "    self.norm_2 = LayerNorm(cfg_map['emb_dim'])\n",
        "    self.ffw = FeedForward(cfg_map)\n",
        "  def forward(self, x):\n",
        "    shortcut_x  = x\n",
        "    x = self.norm_1(x)\n",
        "    x = self.multihead_attention(x)\n",
        "    x = self.dropout(x)\n",
        "    x = shortcut_x + x\n",
        "\n",
        "    shortcut_x = x\n",
        "    x = self.norm_2(x)\n",
        "    x = self.ffw(x)\n",
        "    x = self.dropout(x)\n",
        "    x = x + shortcut_x\n",
        "    return x\n",
        "\n",
        "class LayerNorm(nn.Module):\n",
        "  def __init__(self, emb_dim, eps=1e-5):\n",
        "    super().__init__()\n",
        "    self.epsilon = eps\n",
        "\n",
        "    # learnable params to tweak the layer norm\n",
        "    self.scale = nn.Parameter(torch.ones(emb_dim))\n",
        "    self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
        "\n",
        "  def forward(self, x):\n",
        "    mean = torch.mean(x, dim = -1, keepdim = True)\n",
        "    var = torch.var(x, dim = -1, keepdim = True, correction = False)\n",
        "    norm_x = (x - mean) / (var + self.epsilon)**0.5\n",
        "    return self.scale * norm_x + self.shift\n",
        "\n",
        "class GELULayer(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "  def forward(self, x):\n",
        "    return 0.5 * x * (1 + torch.tanh((2/torch.pi)**0.5 * (x + 0.044715 * x**3)))\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "  def __init__(self, cfg):\n",
        "    super().__init__()\n",
        "    self.layers = nn.Sequential(\n",
        "        nn.Linear(cfg['emb_dim'], 4 * cfg['emb_dim']), # domain expansion\n",
        "        GELULayer(), # just gelu for non-linear\n",
        "        nn.Linear(4 * cfg['emb_dim'], cfg['emb_dim']), # domain contraction\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.layers(x)\n",
        "\n",
        "class MultiheadAttention(nn.Module):\n",
        "  def __init__(self, d_in, d_out, drop_out_rate, context_length, num_heads, ena_bias = False):\n",
        "    super().__init__()\n",
        "\n",
        "    assert (d_out % num_heads == 0), \\\n",
        "      \"d_out must be divisible by num_heads\"\n",
        "\n",
        "    self.d_in = d_in\n",
        "    self.d_out = d_out\n",
        "    self.num_heads = num_heads\n",
        "    self.head_dim = self.d_out // self.num_heads\n",
        "\n",
        "    self.W_Q = nn.Linear(d_in, d_out, bias = ena_bias)\n",
        "    self.W_K = nn.Linear(d_in, d_out, bias = ena_bias)\n",
        "    self.W_V = nn.Linear(d_in, d_out, bias = ena_bias)\n",
        "\n",
        "    # projection?\n",
        "    self.out_proj = nn.Linear(d_out, d_out)\n",
        "\n",
        "    self.drop_out_layer = nn.Dropout(drop_out_rate)\n",
        "    self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal = 1))\n",
        "\n",
        "  def forward(self, x):\n",
        "\n",
        "    batch, num_tokens, d_in = x.shape\n",
        "\n",
        "    queries = self.W_Q(x)\n",
        "    keys = self.W_K(x)\n",
        "    values = self.W_V(x)\n",
        "\n",
        "    queries = queries.view(batch, num_tokens, self.num_heads, self.head_dim)\n",
        "    keys = keys.view(batch, num_tokens, self.num_heads, self.head_dim)\n",
        "    values = values.view(batch, num_tokens, self.num_heads, self.head_dim)\n",
        "\n",
        "    queries = queries.transpose(1,2)\n",
        "    keys = keys.transpose(1,2)\n",
        "    values = values.transpose(1,2)\n",
        "\n",
        "    attention_score = queries @ keys.transpose(2, 3)\n",
        "\n",
        "    mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
        "    attention_score.masked_fill_(mask_bool, -torch.inf)\n",
        "    attention_score = attention_score / self.head_dim**0.5\n",
        "    attention_weight = torch.softmax(attention_score, dim = -1)\n",
        "    attention_weight = self.drop_out_layer(attention_weight)\n",
        "\n",
        "    context_vectors = (attention_weight @ values).transpose(1, 2)\n",
        "\n",
        "    context_vectors = context_vectors.contiguous().view(batch, num_tokens, self.d_out)\n",
        "\n",
        "    # combs for learning relationship of head's results\n",
        "    context_vectors = self.out_proj(context_vectors)\n",
        "\n",
        "    return context_vectors\n"
      ],
      "metadata": {
        "id": "ABo0Toii3v94"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def simple_text_generation(model, idx, context_length, maximum_token):\n",
        "  for i in range(maximum_token):\n",
        "    # slice the input for acceptable input size (<= context length)\n",
        "    idx = idx[:, -context_length:]\n",
        "\n",
        "    with torch.no_grad():\n",
        "      logits = model(idx)\n",
        "\n",
        "    last_vector = logits[:, -1, :]\n",
        "\n",
        "    best_token = torch.argmax(last_vector, dim = -1, keepdim = True)\n",
        "\n",
        "    #print(best_token)\n",
        "\n",
        "    idx = torch.cat((idx, best_token), dim = 1) # (batch, num_token, vocab_size)\n",
        "\n",
        "  return idx\n",
        "\n"
      ],
      "metadata": {
        "id": "ZmGPSPlW3x4x"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "GPT_CONFIG_NEW = {\n",
        "  \"vocab_size\": 50257, # Vocabulary size\n",
        "  \"context_length\": 256, # Context length\n",
        "  \"emb_dim\": 768, # Embedding dimension\n",
        "  \"n_heads\": 12, # Number of attention heads\n",
        "  \"n_layers\": 12, # Number of layers\n",
        "  \"drop_rate\": 0.1, # Dropout rate\n",
        "  \"qkv_bias\": False # Query-Key-Value bias\n",
        "}"
      ],
      "metadata": {
        "id": "6QRcrPwb4I_0"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "\n",
        "def text_to_token_ids(text, tokenizer):\n",
        "  encoded = tokenizer.encode(text,  allowed_special = {'<|endoftext|>'})\n",
        "  encoded_tensor = torch.tensor(encoded).unsqueeze(0) # add batch dim\n",
        "  return encoded_tensor\n",
        "\n",
        "def token_ids_to_text(token_ids, tokenizer):\n",
        "  # tiktoken only accept integer numpy array, never tensor\n",
        "  if isinstance(token_ids, torch.Tensor):\n",
        "      token_ids = token_ids.squeeze().tolist()  # [seq_len]\n",
        "  return tokenizer.decode(token_ids)"
      ],
      "metadata": {
        "id": "6nkxLGHi35_Z"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "model = GPT2Model(GPT_CONFIG_NEW)\n",
        "\n",
        "start_context = \"Every effort moves you\"\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "token_ids = simple_text_generation(\n",
        "model=model,\n",
        "idx=text_to_token_ids(start_context, tokenizer),\n",
        "maximum_token=10,\n",
        "context_length=GPT_CONFIG_NEW[\"context_length\"]\n",
        ")\n",
        "\n",
        "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ulx4auCVS5n9",
        "outputId": "d9371932-6d26-431b-f929-13adf0f0638b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output text:\n",
            " Every effort moves youodonicle ' directly inflamm honeyopoly Kw ply benefit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = torch.tensor([[16833, 3626, 6100], # [\"every effort moves\",\n",
        "                          [40, 1107, 588]]) # \"I really like\"]\n",
        "\n",
        "targets = torch.tensor([[3626, 6100, 345 ], # [\" effort moves you\",\n",
        "                        [1107, 588, 11311]]) # \" really like chocolate\"]"
      ],
      "metadata": {
        "id": "jU1XkBgWlbfK"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad(): # disable training\n",
        "  logits = model(inputs)\n",
        "probas = torch.softmax(logits, dim=-1)\n",
        "print(probas.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S7ipqUH5ljZj",
        "outputId": "b0ea93a5-1ed8-456f-9616-beb5c67ed8ad"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 3, 50257])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# predicted token ids\n",
        "token_ids = torch.argmax(probas, dim=-1, keepdim=True)\n",
        "print(\"Token IDs:\\n\", token_ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pwd6AwuJlu6W",
        "outputId": "03b30339-af78-48a5-ec27-b654b5e79755"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token IDs:\n",
            " tensor([[[24851],\n",
            "         [  406],\n",
            "         [17670]],\n",
            "\n",
            "        [[29716],\n",
            "         [48014],\n",
            "         [44693]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Targets batch 1: {token_ids_to_text(targets[0], tokenizer)}\")\n",
        "print(f\"Outputs batch 1:\"\n",
        "f\" {token_ids_to_text(token_ids[0].flatten(), tokenizer)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4nPyLppvlxze",
        "outputId": "04d0e54a-61e9-4acf-c7e6-121b2d289fa0"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Targets batch 1:  effort moves you\n",
            "Outputs batch 1: etti L variants\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "targets[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "20hNCqU1tYU6",
        "outputId": "1a19f1e1-8d63-430e-f177-341d2f0e264d"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([3626, 6100,  345])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "probas"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tRGsveaQte17",
        "outputId": "26734e36-c080-4b41-8784-184ac578e449"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[5.7738e-05, 1.5567e-05, 6.4669e-06,  ..., 1.0048e-05,\n",
              "          2.0194e-05, 1.2653e-05],\n",
              "         [2.4166e-05, 1.5800e-05, 1.4866e-05,  ..., 5.5020e-06,\n",
              "          8.9726e-06, 5.9605e-05],\n",
              "         [1.3420e-05, 1.3131e-05, 2.8548e-05,  ..., 1.6662e-05,\n",
              "          3.3616e-06, 4.7226e-05]],\n",
              "\n",
              "        [[1.8145e-05, 3.0270e-05, 2.1098e-05,  ..., 2.1022e-05,\n",
              "          2.8455e-05, 4.1221e-05],\n",
              "         [9.5000e-06, 3.1086e-05, 1.0641e-05,  ..., 1.5377e-05,\n",
              "          1.9063e-05, 5.9135e-05],\n",
              "         [2.5252e-05, 9.3113e-06, 1.8893e-05,  ..., 9.4687e-06,\n",
              "          4.9631e-06, 9.6070e-06]]])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_idx = 0\n",
        "target_probas_1 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n",
        "print(\"Text 1:\", target_probas_1)\n",
        "text_idx = 1\n",
        "target_probas_2 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n",
        "print(\"Text 2:\", target_probas_2)\n",
        "\n",
        "# text_idx: batch index\n",
        "# [0, 1, 2]: token index\n",
        "# targets[]: take the target probability."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LJPl3xP5l_Im",
        "outputId": "15ec8253-8010-462b-abaf-f0b8f48689d7"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text 1: tensor([3.7068e-06, 2.0301e-05, 1.7742e-05])\n",
            "Text 2: tensor([1.4735e-05, 2.3505e-05, 6.3795e-06])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "log_probas = torch.log(torch.cat((target_probas_1, target_probas_2)))\n",
        "print(log_probas)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x4mVvqJ_mKif",
        "outputId": "e5f15f29-361b-4341-9823-19ecbd1acba0"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([-12.5053, -10.8049, -10.9396, -11.1253, -10.6583, -11.9624])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "avg_log_probas = torch.mean(log_probas)\n",
        "print(avg_log_probas)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fh9FB_yWnXzt",
        "outputId": "bc6d6315-1f62-4d0b-f8ca-713292558dd7"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(-11.3326)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "neg_avg_probas = avg_log_probas * -1\n",
        "neg_avg_probas"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tfitkFHknca5",
        "outputId": "09161adb-f744-409d-a41f-4d4707577671"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(11.3326)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ce_logits = torch.flatten(logits, start_dim = 0, end_dim = 1)\n",
        "ce_logits.shape"
      ],
      "metadata": {
        "id": "_iAmg6KHnhTO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ce_targets = torch.flatten(targets)\n",
        "ce_targets.shape"
      ],
      "metadata": {
        "id": "wKYYVn-ppu5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss = torch.nn.functional.cross_entropy(ce_logits, ce_targets)\n",
        "loss"
      ],
      "metadata": {
        "id": "NgG9v1bRq4Ez"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/the-verdict.txt\", \"r\", encoding = \"utf-8\") as file:\n",
        "    raw_text = file.read()\n",
        "print(len(raw_text))\n",
        "raw_text[:99]"
      ],
      "metadata": {
        "id": "NFa96hQMrIC5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "metadata": {
        "id": "1FiSI6VFz64t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "p = [1, 2, 3, 4, 5, 6, 7]\n",
        "p[0:3]"
      ],
      "metadata": {
        "id": "y-RTYsJz4qjo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GPTDataset(Dataset):\n",
        "  def __init__(self, text, tokenizer, context_length, stride):\n",
        "    self.input_chunks = []\n",
        "    self.output_chunks = []\n",
        "\n",
        "    encoded_text = tokenizer.encode(text)\n",
        "\n",
        "    for i in range(0, len(encoded_text) - context_length, stride):\n",
        "      input = encoded_text[i : i + context_length]\n",
        "      output = encoded_text[i + 1 : i + context_length + 1]\n",
        "\n",
        "      self.input_chunks.append(torch.tensor(input))\n",
        "      self.output_chunks.append(torch.tensor(output))\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.input_chunks)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    return self.input_chunks[index], self.output_chunks[index]"
      ],
      "metadata": {
        "id": "2fzHMIfV0BER"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dataloader_v1(text, context_length, stride, batch_size, shuffle, drop_last, num_workers):\n",
        "  tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "  dataset = GPTDataset(text, tokenizer, context_length, stride)\n",
        "\n",
        "  dataloader = DataLoader(\n",
        "      dataset = dataset,\n",
        "      batch_size = batch_size,\n",
        "      shuffle = shuffle,\n",
        "      drop_last = drop_last,\n",
        "      num_workers = num_workers\n",
        "  )\n",
        "  return dataloader"
      ],
      "metadata": {
        "id": "noce2wxG0Mw_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ratio = 0.8\n",
        "split_idx = int(len(raw_text) * train_ratio)\n",
        "train_text = raw_text[:split_idx]\n",
        "val_text = raw_text[split_idx:]\n",
        "\n",
        "torch.manual_seed(123)"
      ],
      "metadata": {
        "id": "qyOYHU6M6J0C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = create_dataloader_v1(\n",
        "    text = train_text,\n",
        "    context_length = GPT_CONFIG_NEW['context_length'],\n",
        "    stride = GPT_CONFIG_NEW['context_length'],\n",
        "    batch_size = 2,\n",
        "    shuffle = True,\n",
        "    drop_last = True,\n",
        "    num_workers = 0\n",
        ")\n",
        "\n",
        "val_loader = create_dataloader_v1(\n",
        "    text = val_text,\n",
        "    context_length = GPT_CONFIG_NEW['context_length'],\n",
        "    stride = GPT_CONFIG_NEW['context_length'],\n",
        "    batch_size = 2,\n",
        "    shuffle = True,\n",
        "    drop_last = True,\n",
        "    num_workers = 0\n",
        ")"
      ],
      "metadata": {
        "id": "Er--iIzm6oN8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_tokens = len(tokenizer.encode(raw_text))\n",
        "\n",
        "if total_tokens * train_ratio < GPT_CONFIG_NEW['context_length']:\n",
        "  print(\"shit\")\n",
        "else:\n",
        "  print(\"ok\")"
      ],
      "metadata": {
        "id": "2YPG5i117dZx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if total_tokens * (1-train_ratio) < GPT_CONFIG_NEW['context_length']:\n",
        "  print(\"shit\")\n",
        "else:\n",
        "  print(\"ok\")"
      ],
      "metadata": {
        "id": "PAdq9xBJ7xNo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Train loader:\")\n",
        "for x, y in train_loader:\n",
        "  print(x.shape, y.shape)\n",
        "print(\"\\nValidation loader:\")\n",
        "for x, y in val_loader:\n",
        "  print(x.shape, y.shape)"
      ],
      "metadata": {
        "id": "b6__Kdd875rW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_loss_batch(input, target, model, device):\n",
        "  input_batch = input.to(device)\n",
        "  target_batch = target.to(device)\n",
        "\n",
        "  logits = model(input_batch)\n",
        "\n",
        "  loss = torch.nn.functional.cross_entropy(logits.flatten(0,1), target.flatten())\n",
        "\n",
        "  return loss\n",
        "\n",
        "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
        "  total_loss = 0.\n",
        "  if len(data_loader) == 0:\n",
        "    return float(\"nan\")\n",
        "  elif num_batches is None:\n",
        "    num_batches = len(data_loader) # iter all the batch if user didnt specify the 'num_batches'\n",
        "  else:\n",
        "    num_batches = min(num_batches, len(data_loader)) # if user give smaller 'num_batches' than actual length\n",
        "\n",
        "  for i, (input_batch, target_batch) in enumerate(data_loader):\n",
        "    if i < num_batches:\n",
        "      loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
        "\n",
        "      total_loss += loss.item()\n",
        "\n",
        "    else:\n",
        "      break\n",
        "\n",
        "  return total_loss / num_batches"
      ],
      "metadata": {
        "id": "Q7x0vZoZ8oAZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "with torch.no_grad():\n",
        "  train_loss = calc_loss_loader(train_loader, model, device)\n",
        "  val_loss = calc_loss_loader(val_loader, model, device)\n",
        "print(\"Training loss:\", train_loss)\n",
        "print(\"Validation loss:\", val_loss)"
      ],
      "metadata": {
        "id": "9vK-3ORT_Nvb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, train_loader, val_loader, eval_iter, device):\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    train_loss = calc_loss_loader(train_loader, model, device, eval_iter)\n",
        "\n",
        "    val_loss = calc_loss_loader(val_loader, model, device, eval_iter)\n",
        "\n",
        "  model.train()\n",
        "  return train_loss, val_loss"
      ],
      "metadata": {
        "id": "2hd2576e_tPh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
        "  model.eval()\n",
        "  context_length = model.pos_emb_layer.weight.shape[0]\n",
        "  encoded = text_to_token_ids(start_context, tokenizer)\n",
        "  with torch.no_grad():\n",
        "    tokens = simple_text_generation(model, encoded, context_length, 50)\n",
        "\n",
        "  decoded_text = token_ids_to_text(tokens, tokenizer)\n",
        "  print(decoded_text.replace(\"\\n\", \" \"))\n",
        "  model.train()"
      ],
      "metadata": {
        "id": "g11g3ykNq_SS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model_simple(model, tokenizer, device, start_context, train_loader,\n",
        "                       val_loader, eval_iter, train_epochs, eval_freq, optimizer):\n",
        "  train_losses, val_losses, track_tokens_seen = [], [], []\n",
        "  tokens_seen, global_step = 0, -1\n",
        "  for epoch in range(train_epochs):\n",
        "    model.train()\n",
        "    for (input_batch, target_batch) in train_loader:\n",
        "      optimizer.zero_grad()\n",
        "      loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      tokens_seen += input_batch.numel()\n",
        "      global_step += 1\n",
        "\n",
        "      if global_step % eval_freq == 0: #6\n",
        "        train_loss, val_loss = evaluate_model(model, train_loader, val_loader, eval_iter, device)\n",
        "        train_losses.append(train_loss)\n",
        "        val_losses.append(val_loss)\n",
        "        track_tokens_seen.append(tokens_seen)\n",
        "        print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
        "              f\"Train loss {train_loss:.3f}, \"\n",
        "              f\"Val loss {val_loss:.3f}\"\n",
        "        )\n",
        "    generate_and_print_sample( #7\n",
        "    model, tokenizer, device, start_context\n",
        "    )\n",
        "\n",
        "  return train_losses, val_losses, track_tokens_seen"
      ],
      "metadata": {
        "id": "h22R4eMesfLM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "dumb_gpt = GPT2Model(GPT_CONFIG_NEW)\n",
        "start_context = \"My name is\"\n",
        "optimizer = torch.optim.AdamW(dumb_gpt.parameters(), lr = 0.0004, weight_decay = 0.1)\n",
        "\n",
        "train_losses, val_losses, track_tokens_seen = train_model_simple(\n",
        "    model = dumb_gpt, tokenizer = tokenizer, device = device,\n",
        "    start_context = start_context, train_loader = train_loader,\n",
        "    val_loader = val_loader, eval_iter = 5, train_epochs = 10,\n",
        "    eval_freq = 5, optimizer = optimizer\n",
        ")"
      ],
      "metadata": {
        "id": "v6XXMBteuiUk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.ticker import MaxNLocator\n",
        "num_epochs = 10\n",
        "def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):\n",
        "  fig, ax1 = plt.subplots(figsize=(5, 3))\n",
        "  ax1.plot(epochs_seen, train_losses, label=\"Training loss\")\n",
        "  ax1.plot(\n",
        "  epochs_seen, val_losses, linestyle=\"-.\", label=\"Validation loss\"\n",
        "  )\n",
        "  ax1.set_xlabel(\"Epochs\")\n",
        "  ax1.set_ylabel(\"Loss\")\n",
        "  ax1.legend(loc=\"upper right\")\n",
        "  ax1.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
        "  ax2 = ax1.twiny() #1\n",
        "  ax2.plot(tokens_seen, train_losses, alpha=0) #2\n",
        "  ax2.set_xlabel(\"Tokens seen\")\n",
        "  fig.tight_layout()\n",
        "  plt.show()\n",
        "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
        "plot_losses(epochs_tensor, track_tokens_seen, train_losses, val_losses)"
      ],
      "metadata": {
        "id": "cH9OJq2Pvasz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "\n",
        "start_context = \"Every effort moves you\"\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "token_ids = simple_text_generation(\n",
        "model=dumb_gpt,\n",
        "idx=text_to_token_ids(start_context, tokenizer),\n",
        "maximum_token=25,\n",
        "context_length=GPT_CONFIG_NEW[\"context_length\"]\n",
        ")\n",
        "\n",
        "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
      ],
      "metadata": {
        "id": "R8LIVPeO-dsZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate(model, idx, context_length, maximum_token, temperature = 0.0, topk = None, eos_id = None):\n",
        "  for i in range(maximum_token):\n",
        "    # slice the input for acceptable input size (<= context length)\n",
        "    idx = idx[:, -context_length:]\n",
        "\n",
        "    with torch.no_grad():\n",
        "      logits = model(idx)\n",
        "\n",
        "    last_vector = logits[:, -1, :]\n",
        "\n",
        "    if topk is not None:\n",
        "      top_logits, _ = torch.topk(last_vector, topk)\n",
        "\n",
        "      min_val = top_logits[:, -1]\n",
        "\n",
        "      last_vector = torch.where(\n",
        "          last_vector < min_val,\n",
        "          torch.tensor(float('-inf')).to(last_vector.device),\n",
        "          last_vector\n",
        "      )\n",
        "\n",
        "    if temperature > 0.0: #3\n",
        "      last_vector = last_vector / temperature\n",
        "      probs = torch.softmax(last_vector, dim=-1)\n",
        "      idx_next = torch.multinomial(probs, num_samples=1)\n",
        "    else:\n",
        "      idx_next = torch.argmax(last_vector, dim = -1, keepdim = True)\n",
        "\n",
        "    if idx_next == eos_id:\n",
        "      break\n",
        "\n",
        "    idx = torch.cat((idx, idx_next), dim = 1) # (batch, num_token, vocab_size)\n",
        "\n",
        "  return idx\n",
        "\n"
      ],
      "metadata": {
        "id": "xeNGa8AgOb8C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "\n",
        "start_context = \"Plato is drinking\"\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "token_ids = generate(\n",
        "model=dumb_gpt,\n",
        "idx=text_to_token_ids(start_context, tokenizer),\n",
        "maximum_token=25,\n",
        "context_length=GPT_CONFIG_NEW[\"context_length\"],\n",
        "temperature = 1.4,\n",
        "topk = 5,\n",
        ")\n",
        "\n",
        "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
      ],
      "metadata": {
        "id": "ryNSoLUWP9cO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save\n",
        "torch.save(dumb_gpt.state_dict(), \"dumb_gpt.pth\")"
      ],
      "metadata": {
        "id": "3L8vWoMZSnlX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load\n",
        "model = GPT2Model(GPT_CONFIG_NEW)\n",
        "model.load_state_dict(torch.load(\"model.pth\", map_location=device))\n",
        "model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "id": "ZG-W_RChS1J6",
        "outputId": "3b2b2c5b-0bdf-4faf-e8b8-ea8788ba78bf"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'device' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2083052847.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# load\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGPT2Model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGPT_CONFIG_NEW\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"model.pth\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'device' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save({\n",
        "  \"model_state_dict\": model.state_dict(),\n",
        "  \"optimizer_state_dict\": optimizer.state_dict(),\n",
        "  },\n",
        "  \"model_and_optimizer.pth\"\n",
        ")\n",
        "# save both optimizer and model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "QbvTKjxyT25e",
        "outputId": "0416ff7a-b893-4a5a-bc2e-d10f6b7a6acd"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'optimizer' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1238300450.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m torch.save({\n\u001b[1;32m      2\u001b[0m   \u001b[0;34m\"model_state_dict\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0;34m\"optimizer_state_dict\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m   },\n\u001b[1;32m      5\u001b[0m   \u001b[0;34m\"model_and_optimizer.pth\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'optimizer' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load from checkpoint\n",
        "checkpoint = torch.load(\"model_and_optimizer.pth\", map_location=device)\n",
        "model = GPT2Model(GPT_CONFIG_NEW)\n",
        "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-4, weight_decay=0.1)\n",
        "optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
        "model.train();"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "4GCyFzBQT8Fx",
        "outputId": "86dce7cf-1cbf-41f3-ff70-fa0a1aeb4979"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'device' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-92126073.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# load from checkpoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"model_and_optimizer.pth\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGPT2Model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGPT_CONFIG_NEW\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"model_state_dict\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdamW\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5e-4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'device' is not defined"
          ]
        }
      ]
    }
  ]
}