{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Initial Settings:"
      ],
      "metadata": {
        "id": "k1kVVIJSYBBj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "4_sFgHHKXB4l"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class GPT2Model(nn.Module):\n",
        "  def __init__(self, cfg_map):\n",
        "    super().__init__()\n",
        "\n",
        "    # embedding components\n",
        "    self.emb_layer = nn.Embedding(cfg_map['vocab_size'], cfg_map['emb_dim'])\n",
        "    self.pos_emb_layer = nn.Embedding(cfg_map['context_length'], cfg_map['emb_dim'])\n",
        "\n",
        "    # huh\n",
        "    self.dropout = nn.Dropout(cfg_map['drop_rate'])\n",
        "\n",
        "    # transformer\n",
        "    self.trfm_block = nn.Sequential(*[TransformerBlock(cfg_map) for i in range(cfg_map['n_layers'])])\n",
        "\n",
        "    self.final_norm = LayerNorm(cfg_map['emb_dim'])\n",
        "\n",
        "    # convert to logits\n",
        "    self.out_head = nn.Linear(cfg_map['emb_dim'], cfg_map['vocab_size'], bias = False)\n",
        "\n",
        "  def forward(self, in_idx):\n",
        "    batch_size, seq_len = in_idx.shape\n",
        "    tok_embed = self.emb_layer(in_idx)\n",
        "    pos_embed = self.pos_emb_layer(torch.arange(seq_len, device = in_idx.device))\n",
        "    x = tok_embed + pos_embed\n",
        "    x = self.dropout(x)\n",
        "    x = self.trfm_block(x)\n",
        "    x = self.final_norm(x)\n",
        "\n",
        "    logits = self.out_head(x)\n",
        "\n",
        "    return logits\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "  def __init__(self, cfg_map):\n",
        "    super().__init__()\n",
        "    self.norm_1 = LayerNorm(cfg_map['emb_dim'])\n",
        "    self.multihead_attention = MultiheadAttention(cfg_map['emb_dim'], cfg_map['emb_dim'],\n",
        "                                                  cfg_map['drop_rate'], cfg_map['context_length'],\n",
        "                                                  cfg_map['n_heads'], cfg_map['qkv_bias'])\n",
        "    self.dropout = nn.Dropout(cfg_map['drop_rate'])\n",
        "    self.norm_2 = LayerNorm(cfg_map['emb_dim'])\n",
        "    self.ffw = FeedForward(cfg_map)\n",
        "  def forward(self, x):\n",
        "    shortcut_x  = x\n",
        "    x = self.norm_1(x)\n",
        "    x = self.multihead_attention(x)\n",
        "    x = self.dropout(x)\n",
        "    x = shortcut_x + x\n",
        "\n",
        "    shortcut_x = x\n",
        "    x = self.norm_2(x)\n",
        "    x = self.ffw(x)\n",
        "    x = self.dropout(x)\n",
        "    x = x + shortcut_x\n",
        "    return x\n",
        "\n",
        "class LayerNorm(nn.Module):\n",
        "  def __init__(self, emb_dim, eps=1e-5):\n",
        "    super().__init__()\n",
        "    self.epsilon = eps\n",
        "\n",
        "    # learnable params to tweak the layer norm\n",
        "    self.scale = nn.Parameter(torch.ones(emb_dim))\n",
        "    self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
        "\n",
        "  def forward(self, x):\n",
        "    mean = torch.mean(x, dim = -1, keepdim = True)\n",
        "    var = torch.var(x, dim = -1, keepdim = True, correction = False)\n",
        "    norm_x = (x - mean) / (var + self.epsilon)**0.5\n",
        "    return self.scale * norm_x + self.shift\n",
        "\n",
        "class GELULayer(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "  def forward(self, x):\n",
        "    return 0.5 * x * (1 + torch.tanh((2/torch.pi)**0.5 * (x + 0.044715 * x**3)))\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "  def __init__(self, cfg):\n",
        "    super().__init__()\n",
        "    self.layers = nn.Sequential(\n",
        "        nn.Linear(cfg['emb_dim'], 4 * cfg['emb_dim']), # domain expansion\n",
        "        GELULayer(), # just gelu for non-linear\n",
        "        nn.Linear(4 * cfg['emb_dim'], cfg['emb_dim']), # domain contraction\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.layers(x)\n",
        "\n",
        "class MultiheadAttention(nn.Module):\n",
        "  def __init__(self, d_in, d_out, drop_out_rate, context_length, num_heads, ena_bias = False):\n",
        "    super().__init__()\n",
        "\n",
        "    assert (d_out % num_heads == 0), \\\n",
        "      \"d_out must be divisible by num_heads\"\n",
        "\n",
        "    self.d_in = d_in\n",
        "    self.d_out = d_out\n",
        "    self.num_heads = num_heads\n",
        "    self.head_dim = self.d_out // self.num_heads\n",
        "\n",
        "    self.W_Q = nn.Linear(d_in, d_out, bias = ena_bias)\n",
        "    self.W_K = nn.Linear(d_in, d_out, bias = ena_bias)\n",
        "    self.W_V = nn.Linear(d_in, d_out, bias = ena_bias)\n",
        "\n",
        "    # projection?\n",
        "    self.out_proj = nn.Linear(d_out, d_out)\n",
        "\n",
        "    self.drop_out_layer = nn.Dropout(drop_out_rate)\n",
        "    self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal = 1))\n",
        "\n",
        "  def forward(self, x):\n",
        "\n",
        "    batch, num_tokens, d_in = x.shape\n",
        "\n",
        "    queries = self.W_Q(x)\n",
        "    keys = self.W_K(x)\n",
        "    values = self.W_V(x)\n",
        "\n",
        "    queries = queries.view(batch, num_tokens, self.num_heads, self.head_dim)\n",
        "    keys = keys.view(batch, num_tokens, self.num_heads, self.head_dim)\n",
        "    values = values.view(batch, num_tokens, self.num_heads, self.head_dim)\n",
        "\n",
        "    queries = queries.transpose(1,2)\n",
        "    keys = keys.transpose(1,2)\n",
        "    values = values.transpose(1,2)\n",
        "\n",
        "    attention_score = queries @ keys.transpose(2, 3)\n",
        "\n",
        "    mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
        "    attention_score.masked_fill_(mask_bool, -torch.inf)\n",
        "    attention_score = attention_score / self.head_dim**0.5\n",
        "    attention_weight = torch.softmax(attention_score, dim = -1)\n",
        "    attention_weight = self.drop_out_layer(attention_weight)\n",
        "\n",
        "    context_vectors = (attention_weight @ values).transpose(1, 2)\n",
        "\n",
        "    context_vectors = context_vectors.contiguous().view(batch, num_tokens, self.d_out)\n",
        "\n",
        "    # combs for learning relationship of head's results\n",
        "    context_vectors = self.out_proj(context_vectors)\n",
        "\n",
        "    return context_vectors\n"
      ],
      "metadata": {
        "id": "Oa7xOkTpXRBD"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "GPT_CONFIG_124M = {\n",
        "  \"vocab_size\": 50257, # Vocabulary size\n",
        "  \"context_length\": 256, # Context length\n",
        "  \"emb_dim\": 768, # Embedding dimension\n",
        "  \"n_heads\": 12, # Number of multihead_attentionention heads\n",
        "  \"n_layers\": 12, # Number of layers\n",
        "  \"drop_rate\": 0.1, # Dropout rate\n",
        "  \"qkv_bias\": False # Query-Key-Value bias\n",
        "}"
      ],
      "metadata": {
        "id": "SI9aWH4pXS1z"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "\n",
        "def text_to_token_ids(text, tokenizer):\n",
        "  encoded = tokenizer.encode(text,  allowed_special = {'<|endoftext|>'})\n",
        "  encoded_tensor = torch.tensor(encoded).unsqueeze(0) # add batch dim\n",
        "  return encoded_tensor\n",
        "\n",
        "def token_ids_to_text(token_ids, tokenizer):\n",
        "  # tiktoken only accept integer numpy array, never tensor\n",
        "  if isinstance(token_ids, torch.Tensor):\n",
        "      token_ids = token_ids.squeeze().tolist()  # [seq_len]\n",
        "  return tokenizer.decode(token_ids)"
      ],
      "metadata": {
        "id": "h6uurOkOXT20"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate(model, idx, context_length, maximum_token, temperature = 0.0, topk = None, eos_id = None):\n",
        "  for i in range(maximum_token):\n",
        "    # slice the input for acceptable input size (<= context length)\n",
        "    idx = idx[:, -context_length:]\n",
        "\n",
        "    with torch.no_grad():\n",
        "      logits = model(idx)\n",
        "\n",
        "    last_vector = logits[:, -1, :]\n",
        "\n",
        "    if topk is not None:\n",
        "      top_logits, _ = torch.topk(last_vector, topk)\n",
        "\n",
        "      min_val = top_logits[:, -1]\n",
        "\n",
        "      last_vector = torch.where(\n",
        "          last_vector < min_val,\n",
        "          torch.tensor(float('-inf')).to(last_vector.device),\n",
        "          last_vector\n",
        "      )\n",
        "\n",
        "    if temperature > 0.0: #3\n",
        "      last_vector = last_vector / temperature\n",
        "      probs = torch.softmax(last_vector, dim=-1)\n",
        "      idx_next = torch.multinomial(probs, num_samples=1)\n",
        "    else:\n",
        "      idx_next = torch.argmax(last_vector, dim = -1, keepdim = True)\n",
        "\n",
        "    if idx_next == eos_id:\n",
        "      break\n",
        "\n",
        "    idx = torch.cat((idx, idx_next), dim = 1) # (batch, num_token, vocab_size)\n",
        "\n",
        "  return idx\n",
        "\n"
      ],
      "metadata": {
        "id": "lCxZnd5fXcaP"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Just downloading stuffws:"
      ],
      "metadata": {
        "id": "JUJvcowhYE54"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tensorflow>=2.15.0 tqdm>=4.66"
      ],
      "metadata": {
        "id": "ijxymY9WXcws"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib.request\n",
        "url = (\n",
        "  \"https://raw.githubusercontent.com/rasbt/\"\n",
        "  \"LLMs-from-scratch/main/ch05/\"\n",
        "  \"01_main-chapter-code/gpt_download.py\"\n",
        ")\n",
        "filename = url.split('/')[-1]\n",
        "urllib.request.urlretrieve(url, filename)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9w66-eKQX5VP",
        "outputId": "6de3f5f0-3409-4ff8-8db2-d80d2f3e092a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('gpt_download.py', <http.client.HTTPMessage at 0x7e72d471e480>)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gpt_download import download_and_load_gpt2\n",
        "settings, params = download_and_load_gpt2(\n",
        "model_size=\"355M\", models_dir=\"gpt2\"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jq9SBYhzX9rw",
        "outputId": "2d76a4d3-b997-458b-f3e6-435c54f24884"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "checkpoint: 100%|██████████| 77.0/77.0 [00:00<00:00, 117kiB/s]\n",
            "encoder.json: 100%|██████████| 1.04M/1.04M [00:00<00:00, 2.29MiB/s]\n",
            "hparams.json: 100%|██████████| 91.0/91.0 [00:00<00:00, 131kiB/s]\n",
            "model.ckpt.data-00000-of-00001: 100%|██████████| 1.42G/1.42G [02:05<00:00, 11.3MiB/s]\n",
            "model.ckpt.index: 100%|██████████| 10.4k/10.4k [00:00<00:00, 20.2MiB/s]\n",
            "model.ckpt.meta: 100%|██████████| 927k/927k [00:00<00:00, 1.73MiB/s]\n",
            "vocab.bpe: 100%|██████████| 456k/456k [00:00<00:00, 1.78MiB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Settings:\", settings)\n",
        "print(\"Parameter dictionary keys:\", params.keys())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yyVkBbyUYNla",
        "outputId": "e877336b-3192-46ad-f7f8-cc1693b7bfc0"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Settings: {'n_vocab': 50257, 'n_ctx': 1024, 'n_embd': 1024, 'n_head': 16, 'n_layer': 24}\n",
            "Parameter dictionary keys: dict_keys(['blocks', 'b', 'g', 'wpe', 'wte'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(params[\"wte\"])\n",
        "print(\"Token embedding weight tensor dimensions:\", params[\"wte\"].shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9nbwi8fWYYKC",
        "outputId": "ec824f58-97e3-4523-bcbe-93889fda5948"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-0.0115168   0.00311915 -0.00729894 ... -0.05262156 -0.17569277\n",
            "   0.02565791]\n",
            " [-0.00861426  0.06360211 -0.01822355 ... -0.01364703 -0.12153847\n",
            "   0.05352487]\n",
            " [ 0.05854857  0.06891199  0.02622696 ... -0.10057542 -0.19788682\n",
            "  -0.0039184 ]\n",
            " ...\n",
            " [ 0.00162342 -0.04411932 -0.0517492  ... -0.10079621 -0.00865952\n",
            "   0.02637872]\n",
            " [-0.14374605 -0.04632217 -0.00650705 ...  0.07464293 -0.04721651\n",
            "  -0.03829013]\n",
            " [ 0.02065966 -0.01334631 -0.02586888 ...  0.03886637 -0.00233481\n",
            "   0.00107106]]\n",
            "Token embedding weight tensor dimensions: (50257, 1024)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load the setting and params"
      ],
      "metadata": {
        "id": "T3g8YbH4Yl5n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_configs = {\n",
        "  \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
        "  \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
        "  \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
        "  \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
        "}"
      ],
      "metadata": {
        "id": "CmA7e2mOYhJV"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"gpt2-medium (355M)\"\n",
        "NEW_CONFIG = GPT_CONFIG_124M.copy()\n",
        "NEW_CONFIG.update(model_configs[model_name])\n",
        "NEW_CONFIG.update({\"context_length\": 1024})\n",
        "NEW_CONFIG.update({\"qkv_bias\": True})"
      ],
      "metadata": {
        "id": "4z23gUBVYkoW"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gpt2 = GPT2Model(NEW_CONFIG)\n",
        "gpt2.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wrr0focBZ422",
        "outputId": "53e59a39-cf92-4103-e6bb-1fc6e8c7c03f"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPT2Model(\n",
              "  (emb_layer): Embedding(50257, 1024)\n",
              "  (pos_emb_layer): Embedding(1024, 1024)\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (trfm_block): Sequential(\n",
              "    (0): TransformerBlock(\n",
              "      (norm_1): LayerNorm()\n",
              "      (multihead_attention): MultiheadAttention(\n",
              "        (W_Q): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_K): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_V): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (drop_out_layer): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "      (norm_2): LayerNorm()\n",
              "      (ffw): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELULayer()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (1): TransformerBlock(\n",
              "      (norm_1): LayerNorm()\n",
              "      (multihead_attention): MultiheadAttention(\n",
              "        (W_Q): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_K): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_V): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (drop_out_layer): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "      (norm_2): LayerNorm()\n",
              "      (ffw): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELULayer()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (2): TransformerBlock(\n",
              "      (norm_1): LayerNorm()\n",
              "      (multihead_attention): MultiheadAttention(\n",
              "        (W_Q): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_K): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_V): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (drop_out_layer): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "      (norm_2): LayerNorm()\n",
              "      (ffw): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELULayer()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (3): TransformerBlock(\n",
              "      (norm_1): LayerNorm()\n",
              "      (multihead_attention): MultiheadAttention(\n",
              "        (W_Q): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_K): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_V): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (drop_out_layer): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "      (norm_2): LayerNorm()\n",
              "      (ffw): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELULayer()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (4): TransformerBlock(\n",
              "      (norm_1): LayerNorm()\n",
              "      (multihead_attention): MultiheadAttention(\n",
              "        (W_Q): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_K): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_V): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (drop_out_layer): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "      (norm_2): LayerNorm()\n",
              "      (ffw): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELULayer()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (5): TransformerBlock(\n",
              "      (norm_1): LayerNorm()\n",
              "      (multihead_attention): MultiheadAttention(\n",
              "        (W_Q): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_K): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_V): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (drop_out_layer): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "      (norm_2): LayerNorm()\n",
              "      (ffw): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELULayer()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (6): TransformerBlock(\n",
              "      (norm_1): LayerNorm()\n",
              "      (multihead_attention): MultiheadAttention(\n",
              "        (W_Q): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_K): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_V): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (drop_out_layer): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "      (norm_2): LayerNorm()\n",
              "      (ffw): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELULayer()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (7): TransformerBlock(\n",
              "      (norm_1): LayerNorm()\n",
              "      (multihead_attention): MultiheadAttention(\n",
              "        (W_Q): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_K): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_V): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (drop_out_layer): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "      (norm_2): LayerNorm()\n",
              "      (ffw): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELULayer()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (8): TransformerBlock(\n",
              "      (norm_1): LayerNorm()\n",
              "      (multihead_attention): MultiheadAttention(\n",
              "        (W_Q): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_K): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_V): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (drop_out_layer): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "      (norm_2): LayerNorm()\n",
              "      (ffw): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELULayer()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (9): TransformerBlock(\n",
              "      (norm_1): LayerNorm()\n",
              "      (multihead_attention): MultiheadAttention(\n",
              "        (W_Q): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_K): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_V): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (drop_out_layer): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "      (norm_2): LayerNorm()\n",
              "      (ffw): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELULayer()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (10): TransformerBlock(\n",
              "      (norm_1): LayerNorm()\n",
              "      (multihead_attention): MultiheadAttention(\n",
              "        (W_Q): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_K): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_V): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (drop_out_layer): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "      (norm_2): LayerNorm()\n",
              "      (ffw): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELULayer()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (11): TransformerBlock(\n",
              "      (norm_1): LayerNorm()\n",
              "      (multihead_attention): MultiheadAttention(\n",
              "        (W_Q): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_K): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_V): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (drop_out_layer): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "      (norm_2): LayerNorm()\n",
              "      (ffw): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELULayer()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (12): TransformerBlock(\n",
              "      (norm_1): LayerNorm()\n",
              "      (multihead_attention): MultiheadAttention(\n",
              "        (W_Q): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_K): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_V): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (drop_out_layer): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "      (norm_2): LayerNorm()\n",
              "      (ffw): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELULayer()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (13): TransformerBlock(\n",
              "      (norm_1): LayerNorm()\n",
              "      (multihead_attention): MultiheadAttention(\n",
              "        (W_Q): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_K): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_V): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (drop_out_layer): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "      (norm_2): LayerNorm()\n",
              "      (ffw): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELULayer()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (14): TransformerBlock(\n",
              "      (norm_1): LayerNorm()\n",
              "      (multihead_attention): MultiheadAttention(\n",
              "        (W_Q): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_K): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_V): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (drop_out_layer): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "      (norm_2): LayerNorm()\n",
              "      (ffw): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELULayer()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (15): TransformerBlock(\n",
              "      (norm_1): LayerNorm()\n",
              "      (multihead_attention): MultiheadAttention(\n",
              "        (W_Q): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_K): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_V): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (drop_out_layer): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "      (norm_2): LayerNorm()\n",
              "      (ffw): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELULayer()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (16): TransformerBlock(\n",
              "      (norm_1): LayerNorm()\n",
              "      (multihead_attention): MultiheadAttention(\n",
              "        (W_Q): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_K): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_V): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (drop_out_layer): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "      (norm_2): LayerNorm()\n",
              "      (ffw): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELULayer()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (17): TransformerBlock(\n",
              "      (norm_1): LayerNorm()\n",
              "      (multihead_attention): MultiheadAttention(\n",
              "        (W_Q): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_K): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_V): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (drop_out_layer): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "      (norm_2): LayerNorm()\n",
              "      (ffw): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELULayer()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (18): TransformerBlock(\n",
              "      (norm_1): LayerNorm()\n",
              "      (multihead_attention): MultiheadAttention(\n",
              "        (W_Q): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_K): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_V): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (drop_out_layer): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "      (norm_2): LayerNorm()\n",
              "      (ffw): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELULayer()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (19): TransformerBlock(\n",
              "      (norm_1): LayerNorm()\n",
              "      (multihead_attention): MultiheadAttention(\n",
              "        (W_Q): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_K): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_V): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (drop_out_layer): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "      (norm_2): LayerNorm()\n",
              "      (ffw): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELULayer()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (20): TransformerBlock(\n",
              "      (norm_1): LayerNorm()\n",
              "      (multihead_attention): MultiheadAttention(\n",
              "        (W_Q): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_K): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_V): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (drop_out_layer): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "      (norm_2): LayerNorm()\n",
              "      (ffw): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELULayer()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (21): TransformerBlock(\n",
              "      (norm_1): LayerNorm()\n",
              "      (multihead_attention): MultiheadAttention(\n",
              "        (W_Q): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_K): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_V): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (drop_out_layer): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "      (norm_2): LayerNorm()\n",
              "      (ffw): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELULayer()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (22): TransformerBlock(\n",
              "      (norm_1): LayerNorm()\n",
              "      (multihead_attention): MultiheadAttention(\n",
              "        (W_Q): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_K): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_V): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (drop_out_layer): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "      (norm_2): LayerNorm()\n",
              "      (ffw): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELULayer()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (23): TransformerBlock(\n",
              "      (norm_1): LayerNorm()\n",
              "      (multihead_attention): MultiheadAttention(\n",
              "        (W_Q): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_K): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_V): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (drop_out_layer): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "      (norm_2): LayerNorm()\n",
              "      (ffw): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELULayer()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (final_norm): LayerNorm()\n",
              "  (out_head): Linear(in_features=1024, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def assign(left, right):\n",
        "  a = left.shape\n",
        "  print(\" left ok.\")\n",
        "  b = right.shape\n",
        "  print(\" right ok.\")\n",
        "  if left.shape != right.shape:\n",
        "      raise ValueError(f\"Shape mismatch. Left: {left.shape}, \"\n",
        "                        \"Right: {right.shape}\"\n",
        "                        )\n",
        "  return torch.nn.Parameter(torch.tensor(right))"
      ],
      "metadata": {
        "id": "jlampwyAZ7Qj"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def load_weights_into_gpt(gpt, params):\n",
        "  # position embedding layer\n",
        "  gpt.pos_emb_layer.weight = assign(gpt.pos_emb_layer.weight, params['wpe'])\n",
        "  print(\"Position embedding done!\")\n",
        "\n",
        "  # token embedding layer\n",
        "  gpt.emb_layer.weight = assign(gpt.emb_layer.weight, params['wte'])\n",
        "  print(\"Token embedding done!\")\n",
        "\n",
        "  # transformer block:\n",
        "  for b in range(len(params['blocks'])):\n",
        "    # take the weight from params, split it into Q, K, V\n",
        "    q_w, k_w, v_w = np.split((params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"w\"], 3, axis=-1)\n",
        "\n",
        "    # load into our instance\n",
        "    gpt.trfm_block[b].multihead_attention.W_Q.weight = assign(gpt.trfm_block[b].multihead_attention.W_Q.weight, q_w.T)\n",
        "    gpt.trfm_block[b].multihead_attention.W_K.weight = assign(gpt.trfm_block[b].multihead_attention.W_K.weight, k_w.T)\n",
        "    gpt.trfm_block[b].multihead_attention.W_V.weight = assign(gpt.trfm_block[b].multihead_attention.W_V.weight, v_w.T)\n",
        "    print(\"QKV weight done!\")\n",
        "\n",
        "    # bias for Q, K, V\n",
        "    q_b, k_b, v_b = np.split((params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"b\"], 3, axis=-1)\n",
        "    gpt.trfm_block[b].multihead_attention.W_Q.bias = assign(gpt.trfm_block[b].multihead_attention.W_Q.bias, q_b)\n",
        "    gpt.trfm_block[b].multihead_attention.W_K.bias = assign(gpt.trfm_block[b].multihead_attention.W_K.bias, k_b)\n",
        "    gpt.trfm_block[b].multihead_attention.W_V.bias = assign(gpt.trfm_block[b].multihead_attention.W_V.bias, v_b)\n",
        "    print(\"QKV bias done!\")\n",
        "\n",
        "    # weight for outer projection\n",
        "    gpt.trfm_block[b].multihead_attention.out_proj.weight = assign(gpt.trfm_block[b].multihead_attention.out_proj.weight, params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"w\"].T)\n",
        "    gpt.trfm_block[b].multihead_attention.out_proj.bias = assign(gpt.trfm_block[b].multihead_attention.out_proj.bias, params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"b\"])\n",
        "    print(\"Outer projection done!\")\n",
        "\n",
        "    # feed forward\n",
        "    gpt.trfm_block[b].ffw.layers[0].weight = assign(gpt.trfm_block[b].ffw.layers[0].weight, params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"w\"].T)\n",
        "    gpt.trfm_block[b].ffw.layers[0].bias = assign(gpt.trfm_block[b].ffw.layers[0].bias, params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"b\"])\n",
        "    gpt.trfm_block[b].ffw.layers[2].weight = assign(gpt.trfm_block[b].ffw.layers[2].weight, params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"w\"].T)\n",
        "    gpt.trfm_block[b].ffw.layers[2].bias = assign(gpt.trfm_block[b].ffw.layers[2].bias, params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"b\"])\n",
        "    print(\"Feed forward done!\")\n",
        "\n",
        "    # normalize\n",
        "    gpt.trfm_block[b].norm_1.scale = assign(gpt.trfm_block[b].norm_1.scale, params[\"blocks\"][b][\"ln_1\"][\"g\"])\n",
        "    gpt.trfm_block[b].norm_1.shift = assign(gpt.trfm_block[b].norm_1.shift, params[\"blocks\"][b][\"ln_1\"][\"b\"])\n",
        "    gpt.trfm_block[b].norm_2.scale = assign(gpt.trfm_block[b].norm_2.scale, params[\"blocks\"][b][\"ln_2\"][\"g\"])\n",
        "    gpt.trfm_block[b].norm_2.shift = assign(gpt.trfm_block[b].norm_2.shift, params[\"blocks\"][b][\"ln_2\"][\"b\"])\n",
        "    print(\"Norm trf done!\")\n",
        "\n",
        "  # final norm\n",
        "  gpt.final_norm.scale = assign(gpt.final_norm.scale, params[\"g\"])\n",
        "  gpt.final_norm.shift = assign(gpt.final_norm.shift, params[\"b\"])\n",
        "  print(\"Final norm done!\")\n",
        "\n",
        "  # OG GPT2 reuse the token embedding weight\n",
        "  gpt.out_head.weight = assign(gpt.out_head.weight, params[\"wte\"])\n",
        "  print(\"Out head done!\")"
      ],
      "metadata": {
        "id": "EVNkDkcocT_-"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "load_weights_into_gpt(gpt2, params)\n",
        "gpt2.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O0M4mRiyjf9-",
        "outputId": "6853b320-92b2-4537-e35c-f2e2dfd35f94"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " left ok.\n",
            " right ok.\n",
            "Position embedding done!\n",
            " left ok.\n",
            " right ok.\n",
            "Token embedding done!\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            "QKV weight done!\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            "QKV bias done!\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            "Outer projection done!\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            "Feed forward done!\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            "Norm trf done!\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            "QKV weight done!\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            "QKV bias done!\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            "Outer projection done!\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            "Feed forward done!\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            "Norm trf done!\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            "QKV weight done!\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            "QKV bias done!\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            "Outer projection done!\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            "Feed forward done!\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            "Norm trf done!\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            "QKV weight done!\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            "QKV bias done!\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            "Outer projection done!\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            "Feed forward done!\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            "Norm trf done!\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            "QKV weight done!\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            "QKV bias done!\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            "Outer projection done!\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            "Feed forward done!\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            "Norm trf done!\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            "QKV weight done!\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            "QKV bias done!\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            "Outer projection done!\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            "Feed forward done!\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            "Norm trf done!\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            "QKV weight done!\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            "QKV bias done!\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            "Outer projection done!\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            "Feed forward done!\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            "Norm trf done!\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            "QKV weight done!\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            "QKV bias done!\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            "Outer projection done!\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            "Feed forward done!\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            "Norm trf done!\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            "QKV weight done!\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            "QKV bias done!\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            "Outer projection done!\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            "Feed forward done!\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            "Norm trf done!\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            "QKV weight done!\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            "QKV bias done!\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            "Outer projection done!\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            "Feed forward done!\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            "Norm trf done!\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            "QKV weight done!\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            "QKV bias done!\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            "Outer projection done!\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            "Feed forward done!\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            "Norm trf done!\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            "QKV weight done!\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            "QKV bias done!\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            "Outer projection done!\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            "Feed forward done!\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            "Norm trf done!\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            "QKV weight done!\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            "QKV bias done!\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            "Outer projection done!\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            "Feed forward done!\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            "Norm trf done!\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            "QKV weight done!\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            "QKV bias done!\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            "Outer projection done!\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            "Feed forward done!\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            "Norm trf done!\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            "QKV weight done!\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            "QKV bias done!\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            "Outer projection done!\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            "Feed forward done!\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            "Norm trf done!\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            "QKV weight done!\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            "QKV bias done!\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            "Outer projection done!\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            "Feed forward done!\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            "Norm trf done!\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            "QKV weight done!\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            "QKV bias done!\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            "Outer projection done!\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            "Feed forward done!\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            "Norm trf done!\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            "QKV weight done!\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            "QKV bias done!\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            "Outer projection done!\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            "Feed forward done!\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            "Norm trf done!\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            "QKV weight done!\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            "QKV bias done!\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            "Outer projection done!\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            "Feed forward done!\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            "Norm trf done!\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            "QKV weight done!\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            "QKV bias done!\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            "Outer projection done!\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            "Feed forward done!\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            "Norm trf done!\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            "QKV weight done!\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            "QKV bias done!\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            "Outer projection done!\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            "Feed forward done!\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            "Norm trf done!\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            "QKV weight done!\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            "QKV bias done!\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            "Outer projection done!\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            "Feed forward done!\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            "Norm trf done!\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            "QKV weight done!\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            "QKV bias done!\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            "Outer projection done!\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            "Feed forward done!\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            "Norm trf done!\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            "QKV weight done!\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            "QKV bias done!\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            "Outer projection done!\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            "Feed forward done!\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            "Norm trf done!\n",
            " left ok.\n",
            " right ok.\n",
            " left ok.\n",
            " right ok.\n",
            "Final norm done!\n",
            " left ok.\n",
            " right ok.\n",
            "Out head done!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPT2Model(\n",
              "  (emb_layer): Embedding(50257, 1024)\n",
              "  (pos_emb_layer): Embedding(1024, 1024)\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (trfm_block): Sequential(\n",
              "    (0): TransformerBlock(\n",
              "      (norm_1): LayerNorm()\n",
              "      (multihead_attention): MultiheadAttention(\n",
              "        (W_Q): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_K): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_V): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (drop_out_layer): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "      (norm_2): LayerNorm()\n",
              "      (ffw): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELULayer()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (1): TransformerBlock(\n",
              "      (norm_1): LayerNorm()\n",
              "      (multihead_attention): MultiheadAttention(\n",
              "        (W_Q): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_K): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_V): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (drop_out_layer): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "      (norm_2): LayerNorm()\n",
              "      (ffw): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELULayer()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (2): TransformerBlock(\n",
              "      (norm_1): LayerNorm()\n",
              "      (multihead_attention): MultiheadAttention(\n",
              "        (W_Q): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_K): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_V): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (drop_out_layer): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "      (norm_2): LayerNorm()\n",
              "      (ffw): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELULayer()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (3): TransformerBlock(\n",
              "      (norm_1): LayerNorm()\n",
              "      (multihead_attention): MultiheadAttention(\n",
              "        (W_Q): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_K): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_V): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (drop_out_layer): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "      (norm_2): LayerNorm()\n",
              "      (ffw): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELULayer()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (4): TransformerBlock(\n",
              "      (norm_1): LayerNorm()\n",
              "      (multihead_attention): MultiheadAttention(\n",
              "        (W_Q): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_K): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_V): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (drop_out_layer): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "      (norm_2): LayerNorm()\n",
              "      (ffw): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELULayer()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (5): TransformerBlock(\n",
              "      (norm_1): LayerNorm()\n",
              "      (multihead_attention): MultiheadAttention(\n",
              "        (W_Q): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_K): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_V): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (drop_out_layer): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "      (norm_2): LayerNorm()\n",
              "      (ffw): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELULayer()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (6): TransformerBlock(\n",
              "      (norm_1): LayerNorm()\n",
              "      (multihead_attention): MultiheadAttention(\n",
              "        (W_Q): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_K): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_V): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (drop_out_layer): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "      (norm_2): LayerNorm()\n",
              "      (ffw): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELULayer()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (7): TransformerBlock(\n",
              "      (norm_1): LayerNorm()\n",
              "      (multihead_attention): MultiheadAttention(\n",
              "        (W_Q): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_K): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_V): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (drop_out_layer): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "      (norm_2): LayerNorm()\n",
              "      (ffw): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELULayer()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (8): TransformerBlock(\n",
              "      (norm_1): LayerNorm()\n",
              "      (multihead_attention): MultiheadAttention(\n",
              "        (W_Q): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_K): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_V): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (drop_out_layer): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "      (norm_2): LayerNorm()\n",
              "      (ffw): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELULayer()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (9): TransformerBlock(\n",
              "      (norm_1): LayerNorm()\n",
              "      (multihead_attention): MultiheadAttention(\n",
              "        (W_Q): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_K): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_V): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (drop_out_layer): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "      (norm_2): LayerNorm()\n",
              "      (ffw): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELULayer()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (10): TransformerBlock(\n",
              "      (norm_1): LayerNorm()\n",
              "      (multihead_attention): MultiheadAttention(\n",
              "        (W_Q): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_K): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_V): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (drop_out_layer): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "      (norm_2): LayerNorm()\n",
              "      (ffw): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELULayer()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (11): TransformerBlock(\n",
              "      (norm_1): LayerNorm()\n",
              "      (multihead_attention): MultiheadAttention(\n",
              "        (W_Q): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_K): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_V): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (drop_out_layer): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "      (norm_2): LayerNorm()\n",
              "      (ffw): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELULayer()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (12): TransformerBlock(\n",
              "      (norm_1): LayerNorm()\n",
              "      (multihead_attention): MultiheadAttention(\n",
              "        (W_Q): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_K): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_V): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (drop_out_layer): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "      (norm_2): LayerNorm()\n",
              "      (ffw): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELULayer()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (13): TransformerBlock(\n",
              "      (norm_1): LayerNorm()\n",
              "      (multihead_attention): MultiheadAttention(\n",
              "        (W_Q): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_K): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_V): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (drop_out_layer): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "      (norm_2): LayerNorm()\n",
              "      (ffw): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELULayer()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (14): TransformerBlock(\n",
              "      (norm_1): LayerNorm()\n",
              "      (multihead_attention): MultiheadAttention(\n",
              "        (W_Q): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_K): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_V): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (drop_out_layer): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "      (norm_2): LayerNorm()\n",
              "      (ffw): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELULayer()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (15): TransformerBlock(\n",
              "      (norm_1): LayerNorm()\n",
              "      (multihead_attention): MultiheadAttention(\n",
              "        (W_Q): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_K): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_V): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (drop_out_layer): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "      (norm_2): LayerNorm()\n",
              "      (ffw): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELULayer()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (16): TransformerBlock(\n",
              "      (norm_1): LayerNorm()\n",
              "      (multihead_attention): MultiheadAttention(\n",
              "        (W_Q): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_K): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_V): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (drop_out_layer): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "      (norm_2): LayerNorm()\n",
              "      (ffw): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELULayer()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (17): TransformerBlock(\n",
              "      (norm_1): LayerNorm()\n",
              "      (multihead_attention): MultiheadAttention(\n",
              "        (W_Q): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_K): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_V): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (drop_out_layer): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "      (norm_2): LayerNorm()\n",
              "      (ffw): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELULayer()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (18): TransformerBlock(\n",
              "      (norm_1): LayerNorm()\n",
              "      (multihead_attention): MultiheadAttention(\n",
              "        (W_Q): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_K): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_V): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (drop_out_layer): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "      (norm_2): LayerNorm()\n",
              "      (ffw): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELULayer()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (19): TransformerBlock(\n",
              "      (norm_1): LayerNorm()\n",
              "      (multihead_attention): MultiheadAttention(\n",
              "        (W_Q): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_K): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_V): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (drop_out_layer): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "      (norm_2): LayerNorm()\n",
              "      (ffw): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELULayer()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (20): TransformerBlock(\n",
              "      (norm_1): LayerNorm()\n",
              "      (multihead_attention): MultiheadAttention(\n",
              "        (W_Q): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_K): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_V): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (drop_out_layer): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "      (norm_2): LayerNorm()\n",
              "      (ffw): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELULayer()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (21): TransformerBlock(\n",
              "      (norm_1): LayerNorm()\n",
              "      (multihead_attention): MultiheadAttention(\n",
              "        (W_Q): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_K): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_V): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (drop_out_layer): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "      (norm_2): LayerNorm()\n",
              "      (ffw): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELULayer()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (22): TransformerBlock(\n",
              "      (norm_1): LayerNorm()\n",
              "      (multihead_attention): MultiheadAttention(\n",
              "        (W_Q): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_K): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_V): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (drop_out_layer): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "      (norm_2): LayerNorm()\n",
              "      (ffw): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELULayer()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (23): TransformerBlock(\n",
              "      (norm_1): LayerNorm()\n",
              "      (multihead_attention): MultiheadAttention(\n",
              "        (W_Q): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_K): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_V): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (drop_out_layer): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "      (norm_2): LayerNorm()\n",
              "      (ffw): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELULayer()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (final_norm): LayerNorm()\n",
              "  (out_head): Linear(in_features=1024, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "start_context = \"Holden Caufield\"\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "token_ids = generate(\n",
        "  model=gpt2,\n",
        "  idx=text_to_token_ids(start_context, tokenizer),\n",
        "  maximum_token=100,\n",
        "  context_length=GPT_CONFIG_124M[\"context_length\"],\n",
        "  temperature = 3,\n",
        "  topk = 5,\n",
        ")\n",
        "\n",
        "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jxxDYO4zjzwP",
        "outputId": "231ad964-aaa3-49de-9fb0-56f998dde34a"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output text:\n",
            " Holden Caufield's wife was a teacher at a private school, but she didn't know her son, David, until the family's daughter was in kindergarten. \"I had been in the business of raising kids since I got here and I'd always wanted to have one, and I knew I was going to get it someday,\" she said of her husband and son, a senior who played basketball. \"He's the kind of guy who's going to give you the best chance to do that in life and I\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VkvOmmARtZWO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}